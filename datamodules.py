from typing import Union
from itertools import product

import numpy as np
import h5py
import cv2
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl


class SuperResoNC(Dataset):
    def __init__(
        self,
        data_dir: str,
        *,
        max_height=10,
        use_cv2: bool = False,
        input_size: tuple = None,  # used when `use_cv2` is True.
        step: Union[tuple, int] = None,  # used when `use_cv2` is False.
        shift: Union[tuple, int] = (0, 0),  # used when `use_cv2` is False
        data_key: str = "data"
    ):
        """
        A Pytorch Dataset for super-resolution model. The input to the model are the
        altitude and downsampled grid climate data, and the target is the the original
        grid data. Two downsampling methods are available.If `use_cv2` is True, the grid
        will be viewed as an image with 5 channels and downsampled using `cv2.resize`
        method (`interpolation=cv2.INTER_AREA` is used because it's said to be preferred
        for image decimation). In this case, `input_size` parameter must be passed and
        the grid will be downsampled to `input_size`. If `use_cv2` is False, `step` and
        `shift` must be specified, and the downsampled grid will be generated by just
        splicing the original grid with `shift` as starting indexa and `step` as step
        size.

        Args:
            data_dir: str. Path to data file (in `hdf5` format). The hdf5 file must
                contains a `data` dataset of shape
                [num_samples, num_heights, num_channels, height, width], a `mean` dataset
                of shape [num_heights, num_channels], a `std` dataset of shape
                [num_heights, num_channels].
            use_cv2: bool. If `True`, use `cv2.resize` for downsampling. Otherwise,
                use splicing for downsampling.
            input_size: tuple. Size to resize to. Required when `use_cv2` is True.
            step: tuple | int. Step of splicing. Required when `use_cv2` is False.
            shift: tuple | int. Starting index of splicing. Required when `use_cv2` is
                False.
            data_key: str. The key of data in the h5 file.
        """
        super().__init__()
        self.file = h5py.File(data_dir)
        self.len, num_heights = self.file[data_key].shape[0:2]
        self.num_heights = min(num_heights, max_height)
        self.mean = self.file["mean"][:][..., None, None]
        self.std = self.file["std"][:][..., None, None]

        self.use_cv2 = use_cv2
        if use_cv2:
            assert input_size is not None
            self.input_size = input_size
            self.gen_input = self._gen_input_cv2
        else:
            assert step is not None
            self.step = (step, step) if isinstance(step, int) else step
            self.shift = (shift, shift) if isinstance(shift, int) else shift
            self.gen_input = self._gen_input

    def _gen_input_cv2(self, target):
        return np.array(
            [
                cv2.resize(
                    target[i], self.input_size[::-1], interpolation=cv2.INTER_AREA
                )
                for i in range(target.shape[0])
            ]
        )

    def _gen_input(self, target):
        return target[:, self.shift[0] :: self.step[0], self.shift[1] :: self.step[1]]

    def _aug(self, target):
        if np.random.rand() > 0.5:
            target = np.flip(target, axis=1).copy()  # vertical flip
        if np.random.rand() > 0.5:
            target = np.flip(target, axis=2).copy()  # horizontal flip
        return target

    def __len__(self):
        return self.len * self.num_heights

    def __getitem__(self, index):
        height, idx = divmod(index, self.len)
        # target: [5, 15, 14]
        target = (self.file["data"][idx, height] - self.mean[height]) / self.std[height]
        target = self._aug(target)
        input_ = self.gen_input(target)
        # we do not use from_numpy because `np.flip` create new view with negative stride but pytorch tensor does not support negative stride/
        return (
            torch.tensor(height),
            torch.from_numpy(input_).float(),
            torch.from_numpy(target).float(),
        )


class Corner2CenterDataset(Dataset):
    def __init__(
        self,
        data_dir: str,
        seg_num: Union[tuple, int],
        force_square: bool = False,
    ) -> None:
        """
        A Pytorch Dataset for corner-to-center modelling. The input to the model are the
        altitude and climate data of four corners of a grid and the target is the full
        grid. The task of the model is to reconstruct data within and on borders of the
        grid using only data of four corners of the model, and the corner data is also
        included in the target simply for making up a tensor.

        Args:
            data_dir: str. Path to data file (in `hdf5` format). The hdf5 file must
                contains a `data` dataset of shape
                [num_samples, num_heights, num_channels, height, width], a `mean`
                dataset of shape [num_heights, num_channels], a `std` dataset of shape
                [num_heights, num_channels].
            seg_num: tuple | int. The grid will be cut into `seg_num[0]` pieces along
                height and `seg_num[1]` pieces along width. Thus, the grid will consists
                of `(seg_num[0] + 1) * (seg_num[1] + 1)` points. Because the segments
                must be evenly divided, the choice of `seg_num` will influence how
                training data is generated and consequently the length of the dataset.
                Briefly, each combination of upper left position and lower right
                position will be included as long as it can be evenly divided as
                described above, and grids that is not square will be filtered if
                `force_square` is True.
            force_square: bool.
                If `True`, only generate square data grid. Otherwise, all possible grids
                meeting the condition mentioned above are included. Since spatial
                resolution of the original data is square (1km x 1km), maybe it's better
                to leave this parameter to `True`.
        """
        super().__init__()
        self.file = h5py.File(data_dir)
        self.len, self.num_heights = self.file["data"].shape[0:2]
        self.mean = self.file["mean"][:][..., None, None]
        self.std = self.file["std"][:][..., None, None]

        if isinstance(seg_num, int):
            seg_num = (seg_num, seg_num)
        h, w = self.file["data"].shape[-2:]
        # the first element is (i, i), so we drop it.
        xs = [
            (i, j + 1, k)
            for i in range(h)
            for k, j in enumerate(range(i, h, seg_num[0]))
            if k != 0
        ]
        ys = [
            (i, j + 1, k)
            for i in range(w)
            for k, j in enumerate(range(i, w, seg_num[1]))
            if k != 0
        ]
        if force_square:
            self.anchors = [
                (x, y) for x, y in product(xs, ys) if (x[1] - x[0]) == (y[1] - y[0])
            ]
        else:
            self.anchors = list(product(xs, ys))

    def __len__(self):
        return len(self.anchors) * self.len

    def __getitem__(self, index):
        sample_idx, anchor_idx = divmod(index, len(self.anchors))
        x, y = self.anchors[anchor_idx]
        height = np.random.choice(self.num_heights)
        data = torch.from_numpy(
            (self.file["data"][sample_idx, height] - self.mean[height])
            / self.std[height]
        )
        input_ = data[:, (x[0], x[1])][..., (y[0], y[1])]
        target = data[:, x[0] : x[1] : x[2], y[0] : y[1] : y[2]]
        return torch.tensor(height), input_.float(), target.float()

    def __del__(self):
        self.file.close()


def test_dataset1():
    data_dir = "./data/processed/data.h5"
    dataset = SuperResoNC(data_dir, use_cv2=True, input_size=(10, 8))
    rprint(len(dataset))  # 8736
    rprint(dataset[1234][1].shape)  # (5, *input_size)

    dataset = SuperResoNC(data_dir, use_cv2=False, step=(3, 3), shift=(0, 0))
    rprint(len(dataset))  # 8736
    rprint(dataset[1234][1].shape)  # (5, 8, 7) for `step = 2`, `shift = 0`


def test_dataset2():
    data_dir = "./data/processed/data.h5"
    dataset = Corner2CenterDataset(data_dir, seg_num=2, force_square=True)
    rprint(len(dataset))  # 3546816 for `seg_num=2`, `force_square=True`
    rprint(dataset[1234][1].shape)  # (5, 2, 2)
    rprint(dataset[1233][2].shape)  # (5, seg_num + 1, seg_num + 1)


class SuperResoNCDataModule(pl.LightningDataModule):
    def __init__(
        self, data_dir, batch_size, use_cv2, input_size=None, step=None, shift=None
    ):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.use_cv2 = use_cv2
        self.input_size = input_size
        self.step = step
        self.shift = shift

    def prepare_data(self) -> None:
        with h5py.File(self.data_dir, "a") as f:
            if "data_train" not in f:
                train_data, val_data = train_test_split(f["data"][:], test_size=0.15)
                f["data_train"] = train_data
                f["data_val"] = val_data

    def setup(self, stage=None):
        if stage == "fit":
            self.train_dataset = SuperResoNC(
                self.data_dir,
                use_cv2=self.use_cv2,
                input_size=self.input_size,
                step=self.step,
                shift=self.shift,
                data_key="data_train",
            )
            self.val_dataset = SuperResoNC(
                self.data_dir,
                use_cv2=self.use_cv2,
                input_size=self.input_size,
                step=self.step,
                shift=self.shift,
                data_key="data_val",
            )
        elif stage == "test":
            raise NotImplementedError

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=8,
            pin_memory=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=8,
            pin_memory=True,
        )


if __name__ == "__main__":
    from rich import print as rprint
    from rich.traceback import install

    install()
    test_dataset1()
    # test_dataset2()
