from typing import Union
from itertools import product

import numpy as np
import h5py
import cv2
import torch
from torch.utils.data import Dataset


class SuperResoNC(Dataset):
    def __init__(
        self,
        data_dir: str,
        *,
        use_cv2: bool = False,
        input_size: tuple = None,  # used when `use_cv2` is True.
        step: Union[tuple, int] = None,  # used when `use_cv2` is False.
        shift: Union[tuple, int] = (0, 0),  # used when `use_cv2` is False
    ):
        """
        A Pytorch Dataset for super-resolution model. The input to the model are the
        altitude and downsampled grid climate data, and the target is the the original
        grid data. Two downsampling methods are available.If `use_cv2` is True, the grid
        will be viewed as an image with 5 channels and downsampled using `cv2.resize`
        method (`interpolation=cv2.INTER_AREA` is used because it's said to be preferred
        for image decimation). In this case, `input_size` parameter must be passed and
        the grid will be downsampled to `input_size`. If `use_cv2` is False, `step` and
        `shift` must be specified, and the downsampled grid will be generated by just
        splicing the original grid with `shift` as starting indexa and `step` as step
        size.

        Args:
            data_dir: str. Path to data file (in `hdf5` format). The hdf5 file must
                contains a `data` dataset of shape
                [num_samples, num_heights, num_channels, height, width], a `mean` dataset
                of shape [num_heights, num_channels], a `std` dataset of shape
                [num_heights, num_channels].
            use_cv2: bool. If `True`, use `cv2.resize` for downsampling. Otherwise,
                use splicing for downsampling.
            input_size: tuple. Size to resize to. Required when `use_cv2` is True.
            step: tuple | int. Step of splicing. Required when `use_cv2` is False.
            shift: tuple | int. Starting index of splicing. Required when `use_cv2` is
                False.
        """
        super().__init__()
        self.file = h5py.File(data_dir)
        self.len, self.height = self.file["data"].shape[0:2]
        self.mean = self.file["mean"][:][..., None, None]
        self.std = self.file["std"][:][..., None, None]

        self.use_cv2 = use_cv2
        if use_cv2:
            assert input_size is not None
            self.input_size = input_size
            self.gen_input = self._gen_input_cv2
        else:
            assert step is not None
            self.step = (step, step) if isinstance(step, int) else step
            self.shift = (shift, shift) if isinstance(shift, int) else step
            self.gen_input = self._gen_input

    def _gen_input_cv2(self, target):
        return np.array(
            [
                cv2.resize(
                    target[i], self.input_size[::-1], interpolation=cv2.INTER_AREA
                )
                for i in range(target.shape[0])
            ]
        )

    def _gen_input(self, target):
        return target[:, self.shift[0] :: self.step[0], self.shift[1] :: self.step[1]]

    def __len__(self):
        return self.len

    def __getitem__(self, index):
        height = np.random.choice(self.height)
        # target: [5, 15, 14]
        target = (
            (self.file["data"][index, height] - self.mean[height]) / self.std[height]
        )
        input_ = self.gen_input(target)
        return height, torch.from_numpy(input_), torch.from_numpy(target)

    def __del__(self):
        self.file.close()


class Corner2CenterDataset(Dataset):
    def __init__(
        self,
        data_dir: str,
        seg_num: Union[tuple, int],
        force_square: bool = False,
    ) -> None:
        """
        A Pytorch Dataset for corner-to-center modelling. The input to the model are the
        altitude and climate data of four corners of a grid and the target is the full
        grid. The task of the model is to reconstruct data within and on borders of the
        grid using only data of four corners of the model, and the corner data is also
        included in the target simply for making up a tensor.

        Args:
            data_dir: str. Path to data file (in `hdf5` format). The hdf5 file must
                contains a `data` dataset of shape
                [num_samples, num_heights, num_channels, height, width], a `mean`
                dataset of shape [num_heights, num_channels], a `std` dataset of shape
                [num_heights, num_channels].
            seg_num: tuple | int. The grid will be cut into `seg_num[0]` pieces along
                height and `seg_num[1]` pieces along width. Thus, the grid will consists
                of `(seg_num[0] + 1) * (seg_num[1] + 1)` points. Because the segments
                must be evenly divided, the choice of `seg_num` will influence how
                training data is generated and consequently the length of the dataset.
                Briefly, each combination of upper left position and lower right
                position will be included as long as it can be evenly divided as
                described above, and grids that is not square will be filtered if
                `force_square` is True.
            force_square: bool.
                If `True`, only generate square data grid. Otherwise, all possible grids
                meeting the condition mentioned above are included. Since spatial
                resolution of the original data is square (1km x 1km), maybe it's better
                to leave this parameter to `True`.
        """
        super().__init__()
        self.file = h5py.File(data_dir)
        self.len, self.height = self.file["data"].shape[0:2]
        self.mean = self.file["mean"][:][..., None, None]
        self.std = self.file["std"][:][..., None, None]

        if isinstance(seg_num, int):
            seg_num = (seg_num, seg_num)
        h, w = self.file["data"].shape[-2:]
        # the first element is (i, i), so we drop it.
        xs = [
            (i, j + 1, k)
            for i in range(h)
            for k, j in enumerate(range(i, h, seg_num[0]))
            if k != 0
        ]
        ys = [
            (i, j + 1, k)
            for i in range(w)
            for k, j in enumerate(range(i, w, seg_num[1]))
            if k != 0
        ]
        if force_square:
            self.anchors = [
                (x, y) for x, y in product(xs, ys) if (x[1] - x[0]) == (y[1] - y[0])
            ]
        else:
            self.anchors = list(product(xs, ys))

    def __len__(self):
        return len(self.anchors) * self.len

    def __getitem__(self, index):
        sample_idx, anchor_idx = divmod(index, len(self.anchors))
        x, y = self.anchors[anchor_idx]
        height = np.random.choice(self.height)
        data = torch.from_numpy(
            (self.file["data"][sample_idx, height] - self.mean[height])
            / self.std[height]
        )
        input_ = data[:, (x[0], x[1])][..., (y[0], y[1])]
        target = data[:, x[0] : x[1] : x[2], y[0] : y[1] : y[2]]
        return height, input_, target

    def __del__(self):
        self.file.close()


def test_dataset1():
    data_dir = "./data/processed/data.h5"
    dataset = SuperResoNC(data_dir, use_cv2=True, input_size=(10, 8))
    rprint(len(dataset))  # 8736
    rprint(dataset[1234][1].shape)  # (5, *input_size)

    dataset = SuperResoNC(data_dir, use_cv2=False, step=2, shift=0)
    rprint(len(dataset))  # 8736
    rprint(dataset[1234][1].shape)  # (5, 8, 7) for `step = 2`, `shift = 0`


def test_dataset2():
    data_dir = "./data/processed/data.h5"
    dataset = Corner2CenterDataset(data_dir, seg_num=2, force_square=True)
    rprint(len(dataset))  # 3546816 for `seg_num=2`, `force_square=True`
    rprint(dataset[1234][1].shape)  # (5, 2, 2)
    rprint(dataset[1233][2].shape)  # (5, seg_num + 1, seg_num + 1)


if __name__ == "__main__":
    from rich import print as rprint
    from rich.traceback import install

    install()
    test_dataset1()
    test_dataset2()
